/**
 * ğŸ”´ 04_lexer_tokenizer ä¸Šç´šå•é¡Œ: æœ€é«˜åº¦å­—å¥è§£æã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè£…
 * 
 * ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å“è³ªã®å­—å¥è§£æã‚¨ãƒ³ã‚¸ãƒ³ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
 * ã“ã®å•é¡Œã§ã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã€å›½éš›åŒ–å¯¾å¿œã€ãƒ—ãƒ©ã‚°ã‚¤ãƒ³æ‹¡å¼µæ©Ÿèƒ½ã‚’å­¦ã³ã¾ã™ã€‚
 */

// ===== æœ€é©åŒ–å­—å¥è§£æã‚¨ãƒ³ã‚¸ãƒ³ =====

export class OptimizedLexer {
  private tokenPool: TokenPool = new TokenPool();
  private lookupTable: Map<string, AdvancedTokenType> = new Map();
  private stateTable: LexerStateTable = new LexerStateTable();
  private metrics: LexerMetrics = new LexerMetrics();

  constructor() {
    this.buildLookupTable();
    this.initializeStateTable();
  }

  /**
   * é«˜é€Ÿãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™
   */
  private buildLookupTable(): void {
    // TODO: O(1)ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹ç¯‰
    // ãƒ’ãƒ³ãƒˆ1: ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚ˆã‚‹é«˜é€Ÿæ¤œç´¢
    // ãƒ’ãƒ³ãƒˆ2: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®äº‹å‰è¨ˆç®—
    // ãƒ’ãƒ³ãƒˆ3: æ–‡å­—ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚‹åˆ†é¡
    
    // ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«
    const keywords = [
      'IF', 'ELSE', 'ENDIF', 'WHILE', 'ENDWHILE', 'FOR', 'ENDFOR',
      'FUNCTION', 'ENDFUNCTION', 'RETURN', 'TRY', 'CATCH', 'ENDTRY',
      'REPEAT', 'ENDREPEAT', 'SWITCH', 'CASE', 'DEFAULT', 'ENDSWITCH',
      'DEF', 'SAY', 'MOVE', 'GOTO', 'ATTACK', 'DIG', 'PLACE', 'WAIT',
      'TRUE', 'FALSE', 'NULL', 'UNDEFINED'
    ];
    
    for (const keyword of keywords) {
      this.lookupTable.set(keyword.toLowerCase(), AdvancedTokenType.KEYWORD);
    }
  }

  /**
   * çŠ¶æ…‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–ã—ã¾ã™
   */
  private initializeStateTable(): void {
    // TODO: æœ‰é™çŠ¶æ…‹æ©Ÿæ¢°ã«ã‚ˆã‚‹å­—å¥è§£æ
    // ãƒ’ãƒ³ãƒˆ1: çŠ¶æ…‹é·ç§»ãƒ†ãƒ¼ãƒ–ãƒ«
    // ãƒ’ãƒ³ãƒˆ2: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é–¢æ•°
    // ãƒ’ãƒ³ãƒˆ3: äºˆæ¸¬çš„è§£æ
    
    this.stateTable.addState(LexerState.INITIAL);
    this.stateTable.addState(LexerState.IDENTIFIER);
    this.stateTable.addState(LexerState.NUMBER);
    this.stateTable.addState(LexerState.STRING);
    this.stateTable.addState(LexerState.OPERATOR);
    this.stateTable.addState(LexerState.COMMENT);
    
    // çŠ¶æ…‹é·ç§»ã®å®šç¾©
    this.setupStateTransitions();
  }

  /**
   * çŠ¶æ…‹é·ç§»ã‚’è¨­å®šã—ã¾ã™
   */
  private setupStateTransitions(): void {
    const table = this.stateTable;
    
    // è‹±å­— -> è­˜åˆ¥å­çŠ¶æ…‹
    table.addTransition(LexerState.INITIAL, CharacterClass.LETTER, LexerState.IDENTIFIER);
    table.addTransition(LexerState.IDENTIFIER, CharacterClass.ALPHANUMERIC, LexerState.IDENTIFIER);
    
    // æ•°å­— -> æ•°å€¤çŠ¶æ…‹
    table.addTransition(LexerState.INITIAL, CharacterClass.DIGIT, LexerState.NUMBER);
    table.addTransition(LexerState.NUMBER, CharacterClass.DIGIT, LexerState.NUMBER);
    table.addTransition(LexerState.NUMBER, CharacterClass.DOT, LexerState.NUMBER);
    
    // å¼•ç”¨ç¬¦ -> æ–‡å­—åˆ—çŠ¶æ…‹
    table.addTransition(LexerState.INITIAL, CharacterClass.QUOTE, LexerState.STRING);
    
    // ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ -> ã‚³ãƒ¡ãƒ³ãƒˆçŠ¶æ…‹
    table.addTransition(LexerState.INITIAL, CharacterClass.SLASH, LexerState.COMMENT);
  }

  /**
   * é«˜é€Ÿå­—å¥è§£æã‚’å®Ÿè¡Œã—ã¾ã™
   */
  tokenize(input: string): OptimizedLexicalResult {
    // TODO: é«˜é€Ÿå­—å¥è§£æã®å®Ÿè£…
    // ãƒ’ãƒ³ãƒˆ1: ãƒã‚¤ãƒˆå˜ä½ã®å‡¦ç†
    // ãƒ’ãƒ³ãƒˆ2: åˆ†å²äºˆæ¸¬ã®æœ€é©åŒ–
    // ãƒ’ãƒ³ãƒˆ3: ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã®æ´»ç”¨
    
    const startTime = performance.now();
    this.metrics.reset();
    
    const tokens = this.tokenPool.acquire();
    const errors: LexicalError[] = [];
    
    let position = 0;
    let line = 1;
    let column = 1;
    let state = LexerState.INITIAL;
    
    while (position < input.length) {
      const char = input[position];
      const charClass = this.classifyCharacter(char);
      
      const result = this.processCharacter(char, charClass, state, position, line, column);
      
      if (result.token) {
        tokens.push(result.token);
      }
      
      if (result.error) {
        errors.push(result.error);
      }
      
      state = result.nextState;
      position = result.nextPosition;
      
      if (char === '\n') {
        line++;
        column = 1;
      } else {
        column++;
      }
    }
    
    const endTime = performance.now();
    this.metrics.recordTime(endTime - startTime);
    this.metrics.recordTokens(tokens.length);
    
    return new OptimizedLexicalResult(tokens, errors, this.metrics.getSnapshot());
  }

  /**
   * æ–‡å­—ã‚’åˆ†é¡ã—ã¾ã™
   */
  private classifyCharacter(char: string): CharacterClass {
    const code = char.charCodeAt(0);
    
    if (code >= 65 && code <= 90) return CharacterClass.LETTER; // A-Z
    if (code >= 97 && code <= 122) return CharacterClass.LETTER; // a-z
    if (code >= 48 && code <= 57) return CharacterClass.DIGIT; // 0-9
    if (code === 95) return CharacterClass.UNDERSCORE; // _
    if (code === 46) return CharacterClass.DOT; // .
    if (code === 34 || code === 39) return CharacterClass.QUOTE; // " '
    if (code === 47) return CharacterClass.SLASH; // /
    if (code === 32 || code === 9) return CharacterClass.WHITESPACE; // space, tab
    if (code === 10 || code === 13) return CharacterClass.NEWLINE; // \n \r
    
    return CharacterClass.OTHER;
  }

  /**
   * æ–‡å­—ã‚’å‡¦ç†ã—ã¾ã™
   */
  private processCharacter(
    char: string,
    charClass: CharacterClass,
    state: LexerState,
    position: number,
    line: number,
    column: number
  ): ProcessResult {
    // TODO: çŠ¶æ…‹ã«åŸºã¥ãæ–‡å­—å‡¦ç†
    const nextState = this.stateTable.getNextState(state, charClass);
    const action = this.stateTable.getAction(state, charClass);
    
    if (action) {
      return action(char, position, line, column);
    }
    
    return new ProcessResult(nextState, position + 1);
  }
}

// ===== æ–‡å­—åˆ†é¡ =====

export enum CharacterClass {
  LETTER = 'LETTER',
  DIGIT = 'DIGIT',
  ALPHANUMERIC = 'ALPHANUMERIC',
  UNDERSCORE = 'UNDERSCORE',
  DOT = 'DOT',
  QUOTE = 'QUOTE',
  SLASH = 'SLASH',
  WHITESPACE = 'WHITESPACE',
  NEWLINE = 'NEWLINE',
  OTHER = 'OTHER'
}

// ===== å­—å¥è§£æçŠ¶æ…‹ =====

export enum LexerState {
  INITIAL = 'INITIAL',
  IDENTIFIER = 'IDENTIFIER',
  NUMBER = 'NUMBER',
  STRING = 'STRING',
  OPERATOR = 'OPERATOR',
  COMMENT = 'COMMENT',
  ERROR = 'ERROR'
}

// ===== çŠ¶æ…‹ãƒ†ãƒ¼ãƒ–ãƒ« =====

export class LexerStateTable {
  private transitions: Map<string, LexerState> = new Map();
  private actions: Map<string, StateAction> = new Map();
  private states: Set<LexerState> = new Set();

  addState(state: LexerState): void {
    this.states.add(state);
  }

  addTransition(from: LexerState, on: CharacterClass, to: LexerState): void {
    const key = `${from}:${on}`;
    this.transitions.set(key, to);
  }

  addAction(from: LexerState, on: CharacterClass, action: StateAction): void {
    const key = `${from}:${on}`;
    this.actions.set(key, action);
  }

  getNextState(from: LexerState, on: CharacterClass): LexerState {
    const key = `${from}:${on}`;
    return this.transitions.get(key) || from;
  }

  getAction(from: LexerState, on: CharacterClass): StateAction | undefined {
    const key = `${from}:${on}`;
    return this.actions.get(key);
  }
}

export type StateAction = (char: string, position: number, line: number, column: number) => ProcessResult;

export class ProcessResult {
  constructor(
    public nextState: LexerState,
    public nextPosition: number,
    public token?: AdvancedToken,
    public error?: LexicalError
  ) {}
}

// ===== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç† =====

export class TokenPool {
  private pool: AdvancedToken[][] = [];
  private maxPoolSize = 10;

  /**
   * ãƒˆãƒ¼ã‚¯ãƒ³é…åˆ—ã‚’å–å¾—ã—ã¾ã™
   */
  acquire(): AdvancedToken[] {
    return this.pool.pop() || [];
  }

  /**
   * ãƒˆãƒ¼ã‚¯ãƒ³é…åˆ—ã‚’è¿”å´ã—ã¾ã™
   */
  release(tokens: AdvancedToken[]): void {
    if (this.pool.length < this.maxPoolSize) {
      tokens.length = 0; // é…åˆ—ã‚’ã‚¯ãƒªã‚¢
      this.pool.push(tokens);
    }
  }
}

export class LexerMetrics {
  private tokenCount = 0;
  private processingTime = 0;
  private errorCount = 0;
  private cacheHits = 0;
  private cacheMisses = 0;

  reset(): void {
    this.tokenCount = 0;
    this.processingTime = 0;
    this.errorCount = 0;
    this.cacheHits = 0;
    this.cacheMisses = 0;
  }

  recordTime(time: number): void {
    this.processingTime = time;
  }

  recordTokens(count: number): void {
    this.tokenCount = count;
  }

  recordError(): void {
    this.errorCount++;
  }

  recordCacheHit(): void {
    this.cacheHits++;
  }

  recordCacheMiss(): void {
    this.cacheMisses++;
  }

  getSnapshot(): MetricsSnapshot {
    return new MetricsSnapshot(
      this.tokenCount,
      this.processingTime,
      this.errorCount,
      this.cacheHits,
      this.cacheMisses
    );
  }
}

export class MetricsSnapshot {
  constructor(
    public tokenCount: number,
    public processingTime: number,
    public errorCount: number,
    public cacheHits: number,
    public cacheMisses: number
  ) {}

  getTokensPerSecond(): number {
    return this.processingTime > 0 ? this.tokenCount / (this.processingTime / 1000) : 0;
  }

  getCacheHitRate(): number {
    const total = this.cacheHits + this.cacheMisses;
    return total > 0 ? this.cacheHits / total : 0;
  }
}

// ===== å›½éš›åŒ–å¯¾å¿œ =====

export class InternationalLexer extends OptimizedLexer {
  private unicodeCategories: Map<string, UnicodeCategory> = new Map();
  private localizedKeywords: Map<string, Map<string, AdvancedTokenType>> = new Map();

  constructor(locale: string = 'en-US') {
    super();
    this.setupUnicodeSupport();
    this.loadLocalizedKeywords(locale);
  }

  /**
   * Unicodeæ–‡å­—ã‚«ãƒ†ã‚´ãƒªã‚µãƒãƒ¼ãƒˆã‚’è¨­å®šã—ã¾ã™
   */
  private setupUnicodeSupport(): void {
    // TODO: Unicodeæ–‡å­—ã‚«ãƒ†ã‚´ãƒªã®è¨­å®š
    // ãƒ’ãƒ³ãƒˆ1: Unicodeæ–‡å­—ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    // ãƒ’ãƒ³ãƒˆ2: æ­£è¦åŒ–å½¢å¼
    // ãƒ’ãƒ³ãƒˆ3: åŒæ–¹å‘ãƒ†ã‚­ã‚¹ãƒˆ
    
    // åŸºæœ¬çš„ãªUnicodeã‚«ãƒ†ã‚´ãƒª
    this.unicodeCategories.set('L', UnicodeCategory.LETTER);
    this.unicodeCategories.set('N', UnicodeCategory.NUMBER);
    this.unicodeCategories.set('P', UnicodeCategory.PUNCTUATION);
    this.unicodeCategories.set('S', UnicodeCategory.SYMBOL);
    this.unicodeCategories.set('Z', UnicodeCategory.SEPARATOR);
    this.unicodeCategories.set('M', UnicodeCategory.MARK);
    this.unicodeCategories.set('C', UnicodeCategory.OTHER);
  }

  /**
   * ãƒ­ãƒ¼ã‚«ãƒ©ã‚¤ã‚ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã™
   */
  private loadLocalizedKeywords(locale: string): void {
    // TODO: å¤šè¨€èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚µãƒãƒ¼ãƒˆ
    // ãƒ’ãƒ³ãƒˆ1: è¨€èªåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ—
    // ãƒ’ãƒ³ãƒˆ2: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹
    // ãƒ’ãƒ³ãƒˆ3: å‹•çš„èª­ã¿è¾¼ã¿
    
    const englishKeywords = new Map<string, AdvancedTokenType>();
    englishKeywords.set('if', AdvancedTokenType.KEYWORD);
    englishKeywords.set('while', AdvancedTokenType.KEYWORD);
    englishKeywords.set('function', AdvancedTokenType.KEYWORD);
    
    const japaneseKeywords = new Map<string, AdvancedTokenType>();
    japaneseKeywords.set('ã‚‚ã—', AdvancedTokenType.KEYWORD);
    japaneseKeywords.set('ç¹°ã‚Šè¿”ã—', AdvancedTokenType.KEYWORD);
    japaneseKeywords.set('é–¢æ•°', AdvancedTokenType.KEYWORD);
    
    this.localizedKeywords.set('en-US', englishKeywords);
    this.localizedKeywords.set('ja-JP', japaneseKeywords);
  }

  /**
   * Unicodeæ–‡å­—ã‚’åˆ†é¡ã—ã¾ã™
   */
  classifyUnicodeCharacter(char: string): UnicodeCategory {
    // ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
    const code = char.charCodeAt(0);
    
    if (code >= 0x0041 && code <= 0x005A) return UnicodeCategory.LETTER; // A-Z
    if (code >= 0x0061 && code <= 0x007A) return UnicodeCategory.LETTER; // a-z
    if (code >= 0x0030 && code <= 0x0039) return UnicodeCategory.NUMBER; // 0-9
    if (code >= 0x3040 && code <= 0x309F) return UnicodeCategory.LETTER; // ã²ã‚‰ãŒãª
    if (code >= 0x30A0 && code <= 0x30FF) return UnicodeCategory.LETTER; // ã‚«ã‚¿ã‚«ãƒŠ
    if (code >= 0x4E00 && code <= 0x9FAF) return UnicodeCategory.LETTER; // æ¼¢å­—
    
    return UnicodeCategory.OTHER;
  }
}

export enum UnicodeCategory {
  LETTER = 'LETTER',
  NUMBER = 'NUMBER',
  PUNCTUATION = 'PUNCTUATION',
  SYMBOL = 'SYMBOL',
  SEPARATOR = 'SEPARATOR',
  MARK = 'MARK',
  OTHER = 'OTHER'
}

// ===== ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ  =====

export interface LexerPlugin {
  name: string;
  version: string;
  initialize(lexer: PluggableLexer): void;
  tokenize?(input: string, position: number): PluginTokenResult | null;
  preprocess?(input: string): string;
  postprocess?(tokens: AdvancedToken[]): AdvancedToken[];
}

export class PluginTokenResult {
  constructor(
    public token: AdvancedToken,
    public consumedLength: number
  ) {}
}

export class PluggableLexer extends OptimizedLexer {
  private plugins: Map<string, LexerPlugin> = new Map();
  private pluginOrder: string[] = [];

  /**
   * ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ç™»éŒ²ã—ã¾ã™
   */
  registerPlugin(plugin: LexerPlugin): void {
    // TODO: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ç™»éŒ²ã¨ç®¡ç†
    // ãƒ’ãƒ³ãƒˆ1: ä¾å­˜é–¢ä¿‚ã®è§£æ±º
    // ãƒ’ãƒ³ãƒˆ2: åˆæœŸåŒ–é †åº
    // ãƒ’ãƒ³ãƒˆ3: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    
    if (this.plugins.has(plugin.name)) {
      throw new Error(`Plugin ${plugin.name} is already registered`);
    }
    
    this.plugins.set(plugin.name, plugin);
    this.pluginOrder.push(plugin.name);
    
    try {
      plugin.initialize(this);
    } catch (error) {
      this.plugins.delete(plugin.name);
      const index = this.pluginOrder.indexOf(plugin.name);
      if (index >= 0) {
        this.pluginOrder.splice(index, 1);
      }
      throw new Error(`Failed to initialize plugin ${plugin.name}: ${error}`);
    }
  }

  /**
   * ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’å‰Šé™¤ã—ã¾ã™
   */
  unregisterPlugin(name: string): void {
    this.plugins.delete(name);
    const index = this.pluginOrder.indexOf(name);
    if (index >= 0) {
      this.pluginOrder.splice(index, 1);
    }
  }

  /**
   * ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯¾å¿œå­—å¥è§£æ
   */
  tokenizeWithPlugins(input: string): OptimizedLexicalResult {
    // TODO: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒã‚§ãƒ¼ãƒ³ã®å®Ÿè¡Œ
    // ãƒ’ãƒ³ãƒˆ1: å‰å‡¦ç†ãƒ—ãƒ©ã‚°ã‚¤ãƒ³
    // ãƒ’ãƒ³ãƒˆ2: ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆãƒ—ãƒ©ã‚°ã‚¤ãƒ³
    // ãƒ’ãƒ³ãƒˆ3: å¾Œå‡¦ç†ãƒ—ãƒ©ã‚°ã‚¤ãƒ³
    
    let processedInput = input;
    
    // å‰å‡¦ç†ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®å®Ÿè¡Œ
    for (const pluginName of this.pluginOrder) {
      const plugin = this.plugins.get(pluginName);
      if (plugin && plugin.preprocess) {
        processedInput = plugin.preprocess(processedInput);
      }
    }
    
    // åŸºæœ¬å­—å¥è§£æ
    const result = this.tokenize(processedInput);
    let tokens = result.tokens;
    
    // å¾Œå‡¦ç†ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®å®Ÿè¡Œ
    for (const pluginName of this.pluginOrder) {
      const plugin = this.plugins.get(pluginName);
      if (plugin && plugin.postprocess) {
        tokens = plugin.postprocess(tokens);
      }
    }
    
    return new OptimizedLexicalResult(tokens, result.errors, result.metrics);
  }
}

// ===== ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ =====

export class MacroExpansionPlugin implements LexerPlugin {
  name = 'MacroExpansion';
  version = '1.0.0';
  private macros: Map<string, string> = new Map();

  initialize(lexer: PluggableLexer): void {
    // ãƒã‚¯ãƒ­ã®å®šç¾©
    this.macros.set('ATTACK_NEAREST', 'ATTACK nearest_mob');
    this.macros.set('HEAL_SELF', 'USE potion_of_healing');
  }

  preprocess(input: string): string {
    let result = input;
    
    for (const [macro, expansion] of this.macros) {
      const regex = new RegExp(`\\b${macro}\\b`, 'g');
      result = result.replace(regex, expansion);
    }
    
    return result;
  }
}

export class SyntaxHighlightPlugin implements LexerPlugin {
  name = 'SyntaxHighlight';
  version = '1.0.0';

  initialize(lexer: PluggableLexer): void {
    // åˆæœŸåŒ–å‡¦ç†
  }

  postprocess(tokens: AdvancedToken[]): AdvancedToken[] {
    // ãƒˆãƒ¼ã‚¯ãƒ³ã«è‰²æƒ…å ±ã‚’è¿½åŠ 
    return tokens.map(token => {
      const coloredToken = { ...token } as any;
      coloredToken.color = this.getTokenColor(token.type);
      return coloredToken;
    });
  }

  private getTokenColor(type: AdvancedTokenType): string {
    switch (type) {
      case AdvancedTokenType.KEYWORD: return '#569CD6';
      case AdvancedTokenType.STRING: return '#CE9178';
      case AdvancedTokenType.NUMBER: return '#B5CEA8';
      case AdvancedTokenType.COMMENT: return '#6A9955';
      default: return '#D4D4D4';
    }
  }
}

// ===== çµæœã‚¯ãƒ©ã‚¹ =====

export class OptimizedLexicalResult {
  constructor(
    public tokens: AdvancedToken[],
    public errors: LexicalError[],
    public metrics: MetricsSnapshot
  ) {}

  getPerformanceReport(): PerformanceReport {
    return new PerformanceReport(
      this.metrics.getTokensPerSecond(),
      this.metrics.getCacheHitRate(),
      this.errors.length,
      this.tokens.length
    );
  }
}

export class PerformanceReport {
  constructor(
    public tokensPerSecond: number,
    public cacheHitRate: number,
    public errorCount: number,
    public tokenCount: number
  ) {}

  toString(): string {
    return `Performance Report:
  - Tokens/sec: ${this.tokensPerSecond.toFixed(2)}
  - Cache hit rate: ${(this.cacheHitRate * 100).toFixed(2)}%
  - Error count: ${this.errorCount}
  - Token count: ${this.tokenCount}`;
  }
}

// ===== ãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹ =====

export class MasterLexerDemo {
  /**
   * æœ€é«˜åº¦å­—å¥è§£ææ©Ÿèƒ½ã®ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œã—ã¾ã™
   */
  public static runDemo(): void {
    console.log('=== Master Lexer Demo ===');

    // è¤‡é›‘ãªå¤šè¨€èªBotScriptã‚³ãƒ¼ãƒ‰
    const multilingualCode = `
      // English keywords
      FUNCTION calculateHealth(base, modifier)
        DEF result = base * modifier
        IF result <= 0
          RETURN 0
        ENDIF
        RETURN result
      ENDFUNCTION
      
      // Japanese keywords (simulated)
      ã‚‚ã— playerHealth < 50
        SAY "ä½“åŠ›ãŒä½ä¸‹ã—ã¦ã„ã¾ã™"
        USE "å›å¾©è–¬"
      ENDIF
      
      // Unicode identifiers
      DEF ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼å = "å†’é™ºè€…"
      DEF ãƒ€ãƒ¡ãƒ¼ã‚¸å€¤ = 25.5
      
      // Complex expressions
      WHILE (health > 0) && (energy >= 10)
        ATTACK_NEAREST  // ãƒã‚¯ãƒ­å±•é–‹
        WAIT 1.5
      ENDWHILE
    `;

    // æœ€é©åŒ–å­—å¥è§£æå™¨ã®ãƒ†ã‚¹ãƒˆ
    console.log('\n--- Optimized Lexer Test ---');
    const optimizedLexer = new OptimizedLexer();
    const startTime = performance.now();
    
    const result = optimizedLexer.tokenize(multilingualCode);
    const endTime = performance.now();
    
    console.log(`Processing time: ${(endTime - startTime).toFixed(2)}ms`);
    console.log(result.getPerformanceReport().toString());

    // å›½éš›åŒ–å­—å¥è§£æå™¨ã®ãƒ†ã‚¹ãƒˆ
    console.log('\n--- International Lexer Test ---');
    const intlLexer = new InternationalLexer('ja-JP');
    const intlResult = intlLexer.tokenize(multilingualCode);
    
    console.log(`International tokens: ${intlResult.tokens.length}`);
    
    // ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å¯¾å¿œå­—å¥è§£æå™¨ã®ãƒ†ã‚¹ãƒˆ
    console.log('\n--- Pluggable Lexer Test ---');
    const pluggableLexer = new PluggableLexer();
    
    // ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ç™»éŒ²
    pluggableLexer.registerPlugin(new MacroExpansionPlugin());
    pluggableLexer.registerPlugin(new SyntaxHighlightPlugin());
    
    const pluginResult = pluggableLexer.tokenizeWithPlugins(multilingualCode);
    
    console.log(`Plugin-processed tokens: ${pluginResult.tokens.length}`);
    console.log(`First token color: ${(pluginResult.tokens[0] as any).color || 'N/A'}`);

    // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒ
    console.log('\n--- Memory Usage Test ---');
    const memBefore = process.memoryUsage();
    
    // å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    const largeCode = multilingualCode.repeat(1000);
    optimizedLexer.tokenize(largeCode);
    
    const memAfter = process.memoryUsage();
    const memDiff = memAfter.heapUsed - memBefore.heapUsed;
    console.log(`Memory usage: ${(memDiff / 1024 / 1024).toFixed(2)} MB`);

    console.log('\nMaster lexer demo completed');
  }
}

// å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import { AdvancedTokenType, AdvancedToken, LexicalError } from './normal';