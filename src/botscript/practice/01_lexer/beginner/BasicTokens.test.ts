import { BasicLexer, BasicTokenType } from './BasicTokens';

/**
 * ğŸŸ¢ 01_lexer åˆç´šå•é¡Œ1: åŸºæœ¬ãƒˆãƒ¼ã‚¯ãƒ³ã®èªè­˜ - ãƒ†ã‚¹ãƒˆ
 * 
 * ã“ã®ãƒ†ã‚¹ãƒˆãŒå…¨ã¦é€šã‚‹ã‚ˆã†ã« BasicLexer ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚
 */
describe('01_lexer - åˆç´šå•é¡Œ1: åŸºæœ¬ãƒˆãƒ¼ã‚¯ãƒ³ã®èªè­˜', () => {
  
  describe('åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ', () => {
    test('ç©ºæ–‡å­—åˆ—ã®å‡¦ç†', () => {
      const lexer = new BasicLexer('');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(1);
      expect(tokens[0].type).toBe(BasicTokenType.EOF);
    });

    test('SAYã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®èªè­˜', () => {
      const lexer = new BasicLexer('SAY');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(2); // SAY + EOF
      expect(tokens[0]).toEqual({
        type: BasicTokenType.SAY,
        value: 'SAY',
        line: 1,
        column: 1
      });
      expect(tokens[1].type).toBe(BasicTokenType.EOF);
    });

    test('MOVEã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®èªè­˜', () => {
      const lexer = new BasicLexer('MOVE');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(2); // MOVE + EOF
      expect(tokens[0]).toEqual({
        type: BasicTokenType.MOVE,
        value: 'MOVE',
        line: 1,
        column: 1
      });
    });

    test('æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã®èªè­˜', () => {
      const lexer = new BasicLexer('"Hello World"');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(2); // STRING + EOF
      expect(tokens[0]).toEqual({
        type: BasicTokenType.STRING,
        value: 'Hello World', // ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã¯é™¤å»
        line: 1,
        column: 1
      });
    });
  });

  describe('çµ„ã¿åˆã‚ã›ãƒ†ã‚¹ãƒˆ', () => {
    test('SAYã‚³ãƒãƒ³ãƒ‰ã¨æ–‡å­—åˆ—ã®çµ„ã¿åˆã‚ã›', () => {
      const lexer = new BasicLexer('SAY "Hello"');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(3); // SAY + STRING + EOF
      
      expect(tokens[0]).toEqual({
        type: BasicTokenType.SAY,
        value: 'SAY',
        line: 1,
        column: 1
      });
      
      expect(tokens[1]).toEqual({
        type: BasicTokenType.STRING,
        value: 'Hello',
        line: 1,
        column: 5
      });
      
      expect(tokens[2].type).toBe(BasicTokenType.EOF);
    });

    test('è¤‡æ•°ã®ã‚³ãƒãƒ³ãƒ‰ã®çµ„ã¿åˆã‚ã›', () => {
      const lexer = new BasicLexer('SAY "Test" MOVE');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(4); // SAY + STRING + MOVE + EOF
      expect(tokens.map(t => t.type)).toEqual([
        BasicTokenType.SAY,
        BasicTokenType.STRING,
        BasicTokenType.MOVE,
        BasicTokenType.EOF
      ]);
    });
  });

  describe('ç©ºç™½æ–‡å­—ã®å‡¦ç†', () => {
    test('ã‚¹ãƒšãƒ¼ã‚¹ã®ç„¡è¦–', () => {
      const lexer = new BasicLexer('  SAY   "Hello"  ');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(3); // SAY + STRING + EOF
      expect(tokens[0].type).toBe(BasicTokenType.SAY);
      expect(tokens[1].type).toBe(BasicTokenType.STRING);
    });

    test('ã‚¿ãƒ–ã®ç„¡è¦–', () => {
      const lexer = new BasicLexer('\tSAY\t"Test"\t');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(3); // SAY + STRING + EOF
      expect(tokens[0].value).toBe('SAY');
      expect(tokens[1].value).toBe('Test');
    });

    test('æ”¹è¡Œã®å‡¦ç†', () => {
      const lexer = new BasicLexer('SAY\n"Hello"');
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(3); // SAY + STRING + EOF
      expect(tokens[0].line).toBe(1);
      expect(tokens[1].line).toBe(2); // æ”¹è¡Œå¾Œã¯2è¡Œç›®
    });
  });

  describe('ä½ç½®æƒ…å ±ã®æ­£ç¢ºæ€§', () => {
    test('åˆ—ç•ªå·ã®è¿½è·¡', () => {
      const lexer = new BasicLexer('SAY "Test" MOVE');
      const tokens = lexer.tokenize();
      
      expect(tokens[0].column).toBe(1);  // SAY
      expect(tokens[1].column).toBe(5);  // "Test"
      expect(tokens[2].column).toBe(12); // MOVE
    });

    test('è¤‡æ•°è¡Œã§ã®è¡Œç•ªå·è¿½è·¡', () => {
      const lexer = new BasicLexer('SAY\n"Hello"\nMOVE');
      const tokens = lexer.tokenize();
      
      expect(tokens[0].line).toBe(1); // SAY
      expect(tokens[1].line).toBe(2); // "Hello"
      expect(tokens[2].line).toBe(3); // MOVE
    });
  });

  describe('å¤§æ–‡å­—å°æ–‡å­—ã®å‡¦ç†', () => {
    test('å°æ–‡å­—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®èªè­˜', () => {
      const lexer = new BasicLexer('say');
      const tokens = lexer.tokenize();
      
      expect(tokens[0].type).toBe(BasicTokenType.SAY);
      expect(tokens[0].value).toBe('say'); // å…ƒã®å¤§æ–‡å­—å°æ–‡å­—ã‚’ä¿æŒ
    });

    test('æ··åˆã‚±ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®èªè­˜', () => {
      const lexer = new BasicLexer('SaY MoVe');
      const tokens = lexer.tokenize();
      
      expect(tokens[0].type).toBe(BasicTokenType.SAY);
      expect(tokens[1].type).toBe(BasicTokenType.MOVE);
    });
  });

  describe('ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°', () => {
    test('çµ‚ç«¯ã®ãªã„æ–‡å­—åˆ—', () => {
      const lexer = new BasicLexer('"unterminated string');
      
      expect(() => lexer.tokenize()).toThrow(/unterminated/i);
    });

    test('ä¸æ­£ãªæ–‡å­—', () => {
      const lexer = new BasicLexer('SAY @invalid');
      
      expect(() => lexer.tokenize()).toThrow(/unexpected character/i);
    });

    test('ç©ºã®æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«', () => {
      const lexer = new BasicLexer('""');
      const tokens = lexer.tokenize();
      
      expect(tokens[0]).toEqual({
        type: BasicTokenType.STRING,
        value: '', // ç©ºæ–‡å­—åˆ—
        line: 1,
        column: 1
      });
    });
  });

  describe('ãƒãƒ£ãƒ¬ãƒ³ã‚¸ãƒ†ã‚¹ãƒˆ', () => {
    test('é•·ã„æ–‡å­—åˆ—ã®å‡¦ç†', () => {
      const longString = 'This is a very long string that should be processed correctly by the lexer';
      const lexer = new BasicLexer(`"${longString}"`);
      const tokens = lexer.tokenize();
      
      expect(tokens[0].value).toBe(longString);
    });

    test('è¤‡é›‘ãªçµ„ã¿åˆã‚ã›', () => {
      const source = `SAY "Start"
      MOVE
      SAY "End"`;
      
      const lexer = new BasicLexer(source);
      const tokens = lexer.tokenize();
      
      expect(tokens).toHaveLength(6); // SAY + STRING + MOVE + SAY + STRING + EOF
      expect(tokens.filter(t => t.type === BasicTokenType.SAY)).toHaveLength(2);
      expect(tokens.filter(t => t.type === BasicTokenType.STRING)).toHaveLength(2);
      expect(tokens.filter(t => t.type === BasicTokenType.MOVE)).toHaveLength(1);
    });
  });

  describe('æˆåŠŸåˆ¤å®šãƒ†ã‚¹ãƒˆ', () => {
    test('ğŸ‰ å…¨ã¦ã®åŸºæœ¬æ©Ÿèƒ½ãŒå‹•ä½œã™ã‚‹', () => {
      const source = 'SAY "Hello World" MOVE say "goodbye"';
      const lexer = new BasicLexer(source);
      const tokens = lexer.tokenize();
      
      // æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°
      expect(tokens).toHaveLength(6); // SAY + STRING + MOVE + SAY + STRING + EOF
      
      // å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æ­£ç¢ºæ€§
      expect(tokens[0].type).toBe(BasicTokenType.SAY);
      expect(tokens[1].type).toBe(BasicTokenType.STRING);
      expect(tokens[1].value).toBe('Hello World');
      expect(tokens[2].type).toBe(BasicTokenType.MOVE);
      expect(tokens[3].type).toBe(BasicTokenType.SAY);
      expect(tokens[4].type).toBe(BasicTokenType.STRING);
      expect(tokens[4].value).toBe('goodbye');
      expect(tokens[5].type).toBe(BasicTokenType.EOF);
      
      console.log('ğŸ‰ 01_lexer åˆç´šå•é¡Œ1ã‚¯ãƒªã‚¢ï¼åŸºæœ¬çš„ãªå­—å¥è§£æãŒã§ãã¾ã—ãŸï¼');
    });
  });
});